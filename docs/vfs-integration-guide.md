# VFS Integration Guide

**Document Type**: Integration Specification
**Status**: Phase 1 Complete, Phase 2 Roadmap
**Version**: 1.0
**Last Updated**: 2025-11-07
**Audience**: Engineers integrating VFS into Townlet environments

## Executive Summary

The Variable & Feature System (VFS) Phase 1 implementation is **COMPLETE** with 88 passing tests and comprehensive documentation. This guide describes:

1. **Phase 1 Status**: Current capabilities (schema, registry, observation builder)
2. **Integration Patterns**: How to use VFS in training environments
3. **Phase 2 Roadmap**: Behavioral Action Compiler (BAC) integration
4. **Migration Path**: Transitioning from hardcoded to VFS-driven observations

## Phase 1: Current Status ‚úÖ

### Implemented Components

| Component | Status | Tests | Coverage |
|-----------|--------|-------|----------|
| Schema Definitions (VariableDef, ObservationField) | ‚úÖ Complete | 23 | 93% |
| Variable Registry (runtime storage + access control) | ‚úÖ Complete | 25 | 83% |
| Observation Spec Builder (compile-time spec generation) | ‚úÖ Complete | 22 | 92% |
| ActionConfig Extension (reads/writes fields) | ‚úÖ Complete | 14 | 78% |
| Dimension Regression Tests (checkpoint compatibility) | ‚úÖ Complete | 6 | - |
| Integration Tests (end-to-end pipeline) | ‚úÖ Complete | 12 | - |

**Total**: 88 tests passing, ~90% average coverage

### Validated Configs

All 5 curriculum levels validated for dimension compatibility:

| Config | Observation Dims | Status |
|--------|------------------|--------|
| L0_0_minimal | 38 | ‚úÖ Validated |
| L0_5_dual_resource | 78 | ‚úÖ Validated |
| L1_full_observability | 93 | ‚úÖ Validated |
| L2_partial_observability | 54 | ‚úÖ Validated |
| L3_temporal_mechanics | 93 | ‚úÖ Validated |

## Phase 1 Integration Patterns

### Pattern 1: Variable Registry Initialization

**Use Case**: Initialize VFS registry at environment startup

```python
from townlet.vfs import VariableRegistry, VariableDef
import yaml

# Load variable definitions from YAML
with open("configs/L1_full_observability/variables_reference.yaml") as f:
    config = yaml.safe_load(f)

variables = [VariableDef(**var_data) for var_data in config["variables"]]

# Initialize registry
registry = VariableRegistry(
    variables=variables,
    num_agents=population_size,
    device=device,  # torch.device("cuda" if cuda_available else "cpu")
)

# Registry is now ready for get/set operations
```

**Key Points**:
- Registry manages GPU tensors automatically
- Access control enforced at runtime
- Shape management handled by scope semantics

### Pattern 2: Reading Variables (with Access Control)

**Use Case**: Agent network reads meters for decision-making

```python
# Agent reads energy meter (access control enforced)
energy = registry.get("energy", reader="agent")
# Returns: torch.Tensor shape [num_agents] on correct device

# Engine reads position for rendering
position = registry.get("position", reader="engine")
# Returns: torch.Tensor shape [num_agents, 2]

# Attempt unauthorized read (raises PermissionError)
try:
    private_reward = registry.get("internal_motivation", reader="agent")
except PermissionError as e:
    print(f"Access denied: {e}")
```

**Key Points**:
- `reader` parameter enforces access control
- PermissionError raised if access denied
- No runtime overhead (dictionary lookup + permission check)

### Pattern 3: Writing Variables (Environment Dynamics)

**Use Case**: Engine updates meters after action execution

```python
# Update energy after action costs
new_energy = current_energy - action_costs
registry.set("energy", new_energy, writer="engine")

# Update position after movement
new_position = current_position + action_delta
registry.set("position", new_position, writer="actions")

# Attempt unauthorized write (raises PermissionError)
try:
    registry.set("energy", hacked_values, writer="agent")
except PermissionError as e:
    print(f"Write denied: {e}")
```

**Key Points**:
- `writer` parameter enforces write permissions
- Tensors automatically moved to registry device
- No shape validation (caller responsible)

### Pattern 4: Building Observation Specs

**Use Case**: Generate observation spec at environment initialization

```python
from townlet.vfs import VFSObservationSpecBuilder

# Build exposures from config or programmatically
exposures = []
for var in variables:
    if "agent" in var.readable_by:
        entry = {"source_variable": var.id}
        if var.type == "scalar":
            entry["normalization"] = {"kind": "minmax", "min": 0.0, "max": 1.0}
        else:
            entry["normalization"] = None
        exposures.append(entry)

# Build observation spec
builder = VFSObservationSpecBuilder()
obs_spec = builder.build_observation_spec(variables, exposures)

# Calculate total observation dimension
obs_dim = sum(field.shape[0] if field.shape else 1 for field in obs_spec)
print(f"Observation dimension: {obs_dim}")

# Validate against expected dimension (for checkpoint compatibility)
assert obs_dim == EXPECTED_DIM, f"Dimension mismatch! Expected {EXPECTED_DIM}, got {obs_dim}"
```

**Key Points**:
- Compile-time spec generation (no runtime overhead)
- Dimension calculation for network initialization
- Normalization specs preserved for future use

### Pattern 5: Generating Observations from Registry

**Use Case**: Construct agent observations from VFS variables

```python
def get_observations(registry, obs_spec):
    """Generate observations from VFS registry using observation spec."""
    obs_tensors = []

    for field in obs_spec:
        # Read variable from registry
        value = registry.get(field.source_variable, reader="agent")

        # Apply normalization if specified
        if field.normalization:
            value = apply_normalization(value, field.normalization)

        # Flatten if needed (registry tensors are [num_agents] or [num_agents, dims])
        if len(value.shape) == 1:
            value = value.unsqueeze(-1)  # [num_agents] ‚Üí [num_agents, 1]

        obs_tensors.append(value)

    # Concatenate all observation components
    observations = torch.cat(obs_tensors, dim=-1)  # [num_agents, obs_dim]
    return observations
```

**Key Points**:
- Read each field from registry with access control
- Apply normalization as specified in observation spec
- Concatenate into final observation tensor

### Pattern 6: ActionConfig Dependency Tracking

**Use Case**: Declare variable dependencies for actions

```python
from townlet.environment.action_config import ActionConfig
from townlet.vfs import WriteSpec

# Movement action with VFS dependencies
action = ActionConfig(
    id=0,
    name="MOVE_UP",
    type="movement",
    delta=[0, -1],
    costs={"energy": 0.005},
    effects={},
    enabled=True,
    description="Move up one cell",
    icon="‚¨ÜÔ∏è",
    source="substrate",
    source_affordance=None,

    # VFS Integration (Phase 1)
    reads=["position", "energy", "grid_encoding"],  # Variables this action reads
    writes=[
        WriteSpec(
            variable_id="position",
            expression="position + delta",  # Symbolic expression (Phase 2: BAC)
        ),
    ],
)

# Validation: Ensure all read/write variables exist in registry
for var_id in action.reads:
    assert var_id in registry.variables, f"Variable {var_id} not found in registry"

for write_spec in action.writes:
    assert write_spec.variable_id in registry.variables, \
        f"Variable {write_spec.variable_id} not found in registry"
```

**Key Points**:
- Phase 1: Dependency tracking only (not executed)
- Phase 2: BAC will compile expressions into tensor operations
- Enables static analysis of action effects

## Phase 2: Behavioral Action Compiler (BAC) Roadmap

### Overview

**Phase 2** will implement the **Behavioral Action Compiler (BAC)** that compiles VFS expressions into efficient GPU tensor operations.

### BAC Architecture (Planned)

```
Action Definition (YAML)
    ‚Üì
Expression Parser (reads/writes specs)
    ‚Üì
Dependency Analysis (build execution graph)
    ‚Üì
Tensor Operation Compiler (generate PyTorch ops)
    ‚Üì
Batched Execution (apply to all agents in parallel)
```

### BAC Capabilities (Phase 2)

1. **Expression Compilation**
   - Parse symbolic expressions: `position + delta`, `energy - cost`, `health * healing_factor`
   - Generate efficient PyTorch operations
   - Batch operations across all agents

2. **Dependency Resolution**
   - Topological sort of variable dependencies
   - Detect circular dependencies at compile time
   - Optimize execution order for minimal memory footprint

3. **Type Checking**
   - Validate expression types at compile time
   - Ensure shape compatibility (scalar + scalar, vector + vector)
   - Catch type errors before runtime

4. **Effect Composition**
   - Multiple actions can write to same variable (additive effects)
   - Conflict resolution (last-write-wins, additive, multiplicative)
   - Atomic updates for consistency

### Example: BAC Compilation (Phase 2)

**Input** (YAML Action Definition):
```yaml
- id: 5
  name: "INTERACT_BED"
  type: "interaction"
  reads: ["energy", "mood", "interaction_progress"]
  writes:
    - variable_id: "energy"
      expression: "energy + 0.3 * interaction_progress"
    - variable_id: "mood"
      expression: "mood + 0.1 * interaction_progress"
```

**Output** (Compiled Tensor Operations):
```python
# BAC-generated code (Phase 2)
@torch.jit.script
def execute_INTERACT_BED(registry: VariableRegistry, agent_mask: Tensor):
    # Read dependencies
    energy = registry.get("energy", reader="bac")
    mood = registry.get("mood", reader="bac")
    progress = registry.get("interaction_progress", reader="bac")

    # Compute updates (only for agents in mask)
    new_energy = torch.where(
        agent_mask,
        energy + 0.3 * progress,
        energy,  # No change for masked agents
    )
    new_mood = torch.where(
        agent_mask,
        mood + 0.1 * progress,
        mood,
    )

    # Write results
    registry.set("energy", new_energy, writer="bac")
    registry.set("mood", new_mood, writer="bac")
```

### Phase 2 Milestones

| Milestone | Description | Estimated Effort |
|-----------|-------------|------------------|
| **M1**: Expression Parser | Parse arithmetic expressions, variable refs | 2-3 days |
| **M2**: Type System | Static type checking, shape inference | 2-3 days |
| **M3**: Dependency Graph | Topological sort, circular detection | 1-2 days |
| **M4**: Tensor Compiler | Generate PyTorch operations | 3-4 days |
| **M5**: Effect Composition | Multi-action conflict resolution | 2-3 days |
| **M6**: Optimization | JIT compilation, memory optimization | 2-3 days |
| **M7**: Integration Testing | End-to-end validation with training | 2-3 days |

**Total Estimated Effort**: 14-21 days (2-3 weeks)

### Phase 2 Dependencies

**Prerequisites**:
- ‚úÖ VFS Phase 1 complete (schema, registry, observation builder)
- ‚úÖ ActionConfig extended with reads/writes fields
- ‚úÖ Dimension validation passing for all configs

**Required**:
- Expression parser library (consider: `sympy`, `ast`, custom parser)
- Type inference system (leverage Pydantic types)
- Tensor operation templates (PyTorch operations)

## Migration Path: Hardcoded ‚Üí VFS

### Current State (Hardcoded Observations)

**VectorizedHamletEnv** currently uses hardcoded observation concatenation:

```python
def _get_observations(self):
    # Hardcoded substrate encoding
    substrate_obs = self.substrate.get_observation(...)  # 66 dims for L1

    # Hardcoded meters
    meter_values = torch.stack([
        self.meters["energy"],
        self.meters["health"],
        # ... 8 meters total
    ], dim=-1)  # 8 dims

    # Hardcoded affordance at position
    affordance_one_hot = self._get_affordance_at_position()  # 15 dims

    # Hardcoded temporal features
    time_features = torch.stack([
        self.time_sin,
        self.time_cos,
        self.interaction_progress,
        self.lifetime_progress,
    ], dim=-1)  # 4 dims

    # Concatenate (66 + 8 + 15 + 4 = 93 dims for L1)
    return torch.cat([substrate_obs, meter_values, affordance_one_hot, time_features], dim=-1)
```

**Problems**:
- Observation dimension calculation scattered across codebase
- Hard to verify checkpoint compatibility
- Adding/removing observations requires code changes
- No clear dependency tracking

### Target State (VFS-Driven Observations)

**With VFS**, observations are declaratively configured:

```python
def _initialize_vfs(self):
    """Initialize VFS registry and observation spec (once at startup)."""
    # Load variable definitions from config
    variables = load_variables_from_config(self.config_path)

    # Initialize registry
    self.registry = VariableRegistry(
        variables=variables,
        num_agents=self.num_agents,
        device=self.device,
    )

    # Build observation spec
    builder = VFSObservationSpecBuilder()
    self.obs_spec = builder.build_observation_spec(variables, self.exposures)

    # Validate dimension
    obs_dim = sum(field.shape[0] if field.shape else 1 for field in self.obs_spec)
    assert obs_dim == self.expected_obs_dim, \
        f"VFS dimension mismatch! Expected {self.expected_obs_dim}, got {obs_dim}"

def _get_observations(self):
    """Generate observations from VFS registry."""
    obs_tensors = []

    for field in self.obs_spec:
        value = self.registry.get(field.source_variable, reader="agent")

        if field.normalization:
            value = self._apply_normalization(value, field.normalization)

        if len(value.shape) == 1:
            value = value.unsqueeze(-1)

        obs_tensors.append(value)

    return torch.cat(obs_tensors, dim=-1)
```

**Benefits**:
- Observation dimension calculated at compile time (validation)
- Declarative configuration (no code changes for new observations)
- Clear dependency tracking (observation spec)
- Checkpoint compatibility validated by regression tests

### Migration Strategy

**Phase 1.5** (Parallel Systems):
1. Keep current hardcoded observations (production)
2. Add VFS observations as shadow system
3. Compare outputs (should be identical)
4. Validate dimension compatibility
5. Run training experiments with both systems

**Phase 2.0** (BAC Integration):
1. Replace hardcoded observation generation with VFS
2. Implement BAC for action execution
3. Deprecate old meter update logic
4. Full cutover to VFS-driven training

**Phase 2.5** (Optimization):
1. Profile VFS overhead (registry get/set)
2. Optimize hot paths (JIT compilation, caching)
3. Benchmark against hardcoded baseline
4. Tune for production performance

## Best Practices

### 1. Checkpoint Compatibility

**ALWAYS** run dimension regression tests before committing variable changes:

```bash
uv run pytest tests/test_townlet/unit/vfs/test_observation_dimension_regression.py -v
```

**If tests fail**, you've broken checkpoint compatibility. Either:
- Revert variable changes
- Create new config with incremented version
- Document breaking change and migration path

### 2. Access Control Design

**Principle of Least Privilege**: Only grant necessary permissions.

```yaml
# ‚úÖ GOOD: Agent can read energy but not write
- id: "energy"
  readable_by: ["agent", "engine"]
  writable_by: ["engine"]  # Only engine updates energy

# ‚ùå BAD: Agent can write its own energy (cheating)
- id: "energy"
  readable_by: ["agent", "engine"]
  writable_by: ["agent", "engine"]  # Agent can cheat!
```

### 3. Scope Selection

| Use Case | Scope | Example |
|----------|-------|---------|
| Shared state (time, weather) | `global` | `time_sin`, `day_of_week` |
| Per-agent observable state | `agent` | `energy`, `position`, `health` |
| Per-agent hidden state | `agent_private` | `internal_motivation`, `hidden_reward` |

### 4. Type Selection

**Prefer smaller types** when possible:

```yaml
# ‚úÖ GOOD: Use vec2i for grid coordinates
- id: "grid_position"
  type: "vec2i"  # Integer coordinates
  dims: 2

# ‚ùå BAD: Use vecNf for discrete grid coordinates (wastes memory)
- id: "grid_position"
  type: "vecNf"  # Float coordinates on discrete grid
  dims: 2
```

### 5. Normalization Strategy

**Normalize inputs to [0, 1] or [-1, 1]** for stable learning:

```yaml
# ‚úÖ GOOD: Meter normalized to [0, 1]
- id: "obs_energy"
  normalization:
    kind: "minmax"
    min: 0.0
    max: 1.0

# ‚ö†Ô∏è WARNING: Unbounded money (can grow arbitrarily)
- id: "obs_money"
  normalization: null  # Money unbounded - may harm learning!
```

## Testing and Validation

### Unit Tests

```bash
# Schema validation
uv run pytest tests/test_townlet/unit/vfs/test_schema.py -v

# Registry operations
uv run pytest tests/test_townlet/unit/vfs/test_registry.py -v

# Observation builder
uv run pytest tests/test_townlet/unit/vfs/test_observation_builder.py -v

# Dimension regression (CRITICAL)
uv run pytest tests/test_townlet/unit/vfs/test_observation_dimension_regression.py -v
```

### Integration Tests

```bash
# End-to-end pipeline
uv run pytest tests/test_townlet/integration/test_vfs_integration.py -v

# Full VFS suite
uv run pytest tests/test_townlet/unit/vfs/ tests/test_townlet/integration/test_vfs_integration.py -v
```

### Performance Benchmarks (Phase 2)

```python
# Benchmark registry access
def benchmark_registry_get(registry, iterations=10000):
    start = time.time()
    for _ in range(iterations):
        value = registry.get("energy", reader="agent")
    elapsed = time.time() - start
    print(f"Registry get: {elapsed / iterations * 1e6:.2f} ¬µs/call")

# Benchmark observation generation
def benchmark_observation_generation(env, iterations=1000):
    start = time.time()
    for _ in range(iterations):
        obs = env._get_observations()
    elapsed = time.time() - start
    print(f"Observation generation: {elapsed / iterations * 1e3:.2f} ms/call")
```

## Known Limitations and Future Work

### Phase 1 Limitations

1. **No BAC Integration**: Expressions in `writes` are symbolic (not executed)
2. **Manual Observation Generation**: Still need to manually call `registry.get()` and concatenate
3. **No Validation of Expressions**: WriteSpec expressions not validated until Phase 2
4. **Limited Normalization**: Only minmax and zscore supported (no custom functions)

### Future Enhancements (Beyond Phase 2)

1. **Dynamic Variables**: Variables that can be added/removed at runtime
2. **Hierarchical Scopes**: Nested scopes (team ‚Üí agent ‚Üí private)
3. **Variable Versioning**: Track variable schema evolution over time
4. **Observation Caching**: Cache derived observations to reduce recomputation
5. **Multi-Agent Communication**: Shared memory, message passing via VFS
6. **Intrinsic Reward Shaping**: Novelty, curiosity via VFS-tracked state
7. **Curriculum Progression**: Gradually expose variables as agent learns

## Resources

### Documentation
- Configuration guide: `docs/config-schemas/variables.md`
- Design document: `docs/plans/2025-11-06-variables-and-features-system.md`
- Implementation plan: `docs/tasks/TASK-002-variables-and-features-system.md`
- Project guide: `CLAUDE.md` (VFS section)

### Code
- Schema definitions: `src/townlet/vfs/schema.py`
- Variable registry: `src/townlet/vfs/registry.py`
- Observation builder: `src/townlet/vfs/observation_builder.py`
- ActionConfig extension: `src/townlet/environment/action_config.py`

### Tests
- Unit tests: `tests/test_townlet/unit/vfs/`
- Integration tests: `tests/test_townlet/integration/test_vfs_integration.py`
- Regression tests: `tests/test_townlet/unit/vfs/test_observation_dimension_regression.py`

### Reference Configs
- L0_0_minimal: `configs/L0_0_minimal/variables_reference.yaml` (38 dims)
- L0_5_dual_resource: `configs/L0_5_dual_resource/variables_reference.yaml` (78 dims)
- L1_full_observability: `configs/L1_full_observability/variables_reference.yaml` (93 dims)
- L2_partial_observability: `configs/L2_partial_observability/variables_reference.yaml` (54 dims)
- L3_temporal_mechanics: `configs/L3_temporal_mechanics/variables_reference.yaml` (93 dims)

## Contact and Support

For questions or issues with VFS integration:

1. **Check Documentation**: Start with `docs/config-schemas/variables.md`
2. **Review Examples**: Reference variable files in `configs/*/variables_reference.yaml`
3. **Run Tests**: Validate your setup with VFS test suite
4. **Consult Design Doc**: `docs/plans/2025-11-06-variables-and-features-system.md`

## Conclusion

VFS Phase 1 provides a **solid foundation** for declarative state space configuration:

- ‚úÖ **88 tests passing** (unit + integration)
- ‚úÖ **~90% code coverage** across VFS components
- ‚úÖ **Dimension compatibility validated** for all 5 curriculum levels
- ‚úÖ **Comprehensive documentation** (config guide, integration patterns)

**Phase 2 (BAC)** will unlock the full power of VFS by compiling action effects into efficient tensor operations, enabling **true declarative RL environment configuration**.

The migration path is clear, the foundation is solid, and the future is declarative! üöÄ

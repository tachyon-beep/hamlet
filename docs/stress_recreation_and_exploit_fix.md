# Stress Meter, Recreation, and Exploit Fix

**Date**: 2025-10-28
**Changes**: Added stress management mechanics + fixed interact-spam exploit
**Status**: Implemented, ready for training

---

## Overview

Two major improvements to Hamlet:

1. **New Resource Management**: Stress meter + Recreation affordance
2. **Exploit Fix**: Failed interaction penalty (no more standing still spam)

---

## Change 1: Stress Management System

### New Meter: Stress

**File**: `src/hamlet/environment/meters.py`

```python
class Stress(Meter):
    """Stress meter: increases with work, reduced by recreation, slow passive decay."""

    def __init__(self):
        super().__init__(name="stress", initial_value=0.0, min_value=0.0, depletion_rate=-0.1)
```

**Mechanics**:
- Starts at 0 (no stress)
- **Increases** when working (Job: +25 stress)
- **Decreases** at Recreation (-40 stress)
- **Passive decay**: -0.1 per step (slow natural stress relief)
- **Inverted logic**: HIGH stress is bad, LOW stress is good

**Reward Structure**:
```python
# Stress gradient rewards (INVERTED)
if stress_normalized < 0.2:
    reward += 0.5  # Low stress (healthy)
elif stress_normalized < 0.5:
    reward += 0.2  # Moderate stress (manageable)
elif stress_normalized < 0.8:
    reward -= 0.5  # High stress (concerning)
else:
    reward -= 2.0  # Critical stress (take a break!)
```

### New Affordance: Recreation

**File**: `src/hamlet/environment/entities.py`, `affordances.py`

```python
class Recreation(Affordance):
    """Recreation affordance: Money (-), Stress (---), Energy (+)"""

AFFORDANCE_EFFECTS = {
    "Recreation": {
        "money": -8.0,   # Costs money (entertainment/leisure)
        "stress": -40.0, # Significantly reduces stress
        "energy": 10.0,  # Small energy boost (relaxing)
    },
}
```

**Position**: (3, 3) - Center of grid, accessible from all corners
**Icon**: ðŸŽ® (green background)

**Updated Job Effects**:
```python
"Job": {
    "money": 30.0,
    "energy": -15.0,
    "hygiene": -10.0,
    "stress": 25.0,   # NEW: Work increases stress
}
```

### Updated Economics

**Old cycle** (4 affordances):
- Costs: Bed $5 + Shower $3 + Fridge $4 = $12
- Income: Job $30
- Net: +$18/cycle

**New cycle** (5 affordances, with Recreation):
- Costs: Bed + Shower + Fridge + Recreation = $20
- Income: Job $30
- Net: +$10/cycle (sustainable with buffer)

**Recreation is optional but necessary** - agent must balance:
- Work too much â†’ stress builds â†’ need Recreation ($8 + time)
- Skip Recreation â†’ stress critical â†’ penalties
- Strategic tension: spend money now to maintain long-term productivity

---

## Change 2: Fix Interact-Spam Exploit

### The Problem (Pedagogically Valuable!)

**Old behavior**:
- Agent discovered: Standing still + spam INTERACT = optimal
- Movement costs: -0.5 energy, -0.3 hygiene, -0.4 satiation
- Failed INTERACT (no affordance nearby): 0 cost
- Result: Interact-spam while stationary maximized reward

**Why it happened**:
- Proximity rewards for being near affordances
- No penalty for failed interactions
- Agent optimized: "Don't move, just spam interact"

### The Fix

**File**: `src/hamlet/environment/hamlet_env.py`

```python
elif action == self.ACTION_INTERACT:
    # Try to interact with affordance at current position
    cell_contents = self.grid.get_cell_contents(agent.x, agent.y)
    interaction_found = False
    for entity in cell_contents:
        if hasattr(entity, "interact"):
            changes = entity.interact(agent)
            if changes:
                interaction_affordance = entity
                interaction_found = True
            break

    # Penalty for failed interaction (wasted effort)
    if not interaction_found:
        agent.meters.update_all({"energy": -1.0})  # Small cost for failed attempt
```

**Impact**:
- Failed INTERACT costs 1.0 energy
- Makes spamming interact unprofitable
- Agent must navigate properly to affordances
- Preserves pedagogical value as a "great story"

---

## Frontend Updates

### Meter Panel (`frontend/src/components/MeterPanel.vue`)

**Added stress handling**:
- Display as raw value (0-100) instead of percentage
- Inverted coloring: GREEN = low stress (good), RED = high stress (bad)
- Critical threshold: >80% stress (pulses red)

```javascript
function isCritical(name, value) {
  // Stress is inverted - HIGH stress is critical
  if (name === 'stress') {
    return percentage > 80
  }
  return percentage < 20
}

function getMeterColor(name, value) {
  if (name === 'stress') {
    if (percentage < 20) return '#10b981'  // Low stress = green
    if (percentage < 50) return '#f59e0b'  // Medium = yellow
    return '#ef4444'                        // High stress = red
  }
  // ... normal meters
}
```

### Grid Component (`frontend/src/components/Grid.vue`)

**Added Recreation rendering**:
- Icon: ðŸŽ® (gaming controller)
- Colors: Green fill (#10b981), lighter stroke (#34d399)
- Position: Center of grid

---

## Pedagogical Value

### Teaching Moment 1: Multi-Resource Optimization

**Complexity progression**:
- **Level 0**: Single meter (survival)
- **Level 1**: Biological meters (energy, hygiene, satiation)
- **Level 2**: Economic resource (money buffer)
- **Level 3**: Stress management (work-life balance)

**Student learning**:
- Can't optimize each meter independently
- Trade-offs: Work more (money) vs. relax (stress)
- Long-term planning: Prevent stress buildup vs. immediate needs
- Real-world analogy: Work-life balance

### Teaching Moment 2: Inverted Optimization

**Stress is different**:
- Biological meters: Maximize (100% = good)
- Money: Maintain buffer (40-60% = good)
- Stress: Minimize (0% = best)

**Student challenge**: Design reward function for inverted metric
- Gradient rewards flip thresholds
- Proximity shaping needs inverted urgency
- Need-based rewards require special handling

### Teaching Moment 3: The Exploit Story (Preserved!)

**The narrative**:
1. "First agent we trained discovered interact-spam exploit"
2. "Agent stood still, spammed INTERACT, maximized reward"
3. "Technically correct - movement costs more than no-op"
4. "Great example of reward hacking"
5. "Fixed with failed interaction penalty"

**Student takeaway**: Even after fixing interact-spam, agents can find NEW exploits

---

## Expected Agent Behaviors

### Old Agent (No Stress)
- Work when broke
- Use services when needed
- Ignore work-life balance
- Possible death spiral (multi-meter crisis)

### New Agent (With Stress)
Must balance FOUR concerns:
1. **Biological needs** (energy, hygiene, satiation)
2. **Economic buffer** (money for services)
3. **Stress management** (don't overwork)
4. **Strategic timing** (when to work vs. recreate)

### Predicted Strategies

**Strategy A: "Grinder"**
- Work frequently (build money buffer)
- High stress accumulates
- Forced to recreate (costs money + time)
- Cycle: work-work-recreate-work

**Strategy B: "Balanced"**
- Work occasionally (maintain money buffer)
- Stress builds slowly
- Recreate proactively
- Sustainable cycle

**Strategy C: "Avoider"**
- Minimize work (low stress)
- Money runs out
- Death spiral: Can't afford services OR recreation
- Fails

**Optimal**: Likely Strategy B (balanced work-recreation cycles)

---

## State Space Changes

### Old State Representation
- Grid: 8Ã—8 = 64 cells
- Meters: 4 (energy, hygiene, satiation, money)
- Position: (x, y)
- **Total**: 70-dimensional vector

### New State Representation
- Grid: 8Ã—8 = 64 cells (now includes Recreation at value 5.0)
- Meters: 5 (energy, hygiene, satiation, money, stress)
- Position: (x, y)
- **Total**: 71-dimensional vector

**Note**: Existing neural network architectures handle variable meter counts via flattening, so this change is backward compatible with architecture code.

---

## Testing Plan

### 1. Environment Validation
```bash
# Test basic functionality
uv run python -c "
from src.hamlet.environment.hamlet_env import HamletEnv
env = HamletEnv()
obs = env.reset()
print(f'Meters: {list(obs[\"meters\"].keys())}')
print(f'Affordances: {[a.name for a in env.affordances]}')
"
```

**Expected output**:
```
Meters: ['energy', 'hygiene', 'satiation', 'money', 'stress']
Affordances: ['Bed', 'Shower', 'Fridge', 'Job', 'Recreation']
```

### 2. Failed Interaction Penalty
```bash
# Test interact penalty
uv run python -c "
from src.hamlet.environment.hamlet_env import HamletEnv
env = HamletEnv()
env.reset()
agent = env.agents['agent_0']

# Move away from affordances
for _ in range(3):
    env.step(0)  # Move up

# Store energy before failed interact
energy_before = agent.meters.get('energy').value

# Try to interact (should fail - no affordance nearby)
env.step(4)  # INTERACT action

energy_after = agent.meters.get('energy').value
penalty = energy_before - energy_after

print(f'Energy before: {energy_before:.1f}')
print(f'Energy after: {energy_after:.1f}')
print(f'Penalty: {penalty:.1f} (expected: 1.6 = 1.0 failed + 0.5 depletion + 0.1 from movement)')
"
```

### 3. Stress Mechanics
```bash
# Test stress accumulation and decay
uv run python -c "
from src.hamlet.environment.hamlet_env import HamletEnv
env = HamletEnv()
env.reset()
agent = env.agents['agent_0']

print(f'Initial stress: {agent.meters.get(\"stress\").value}')

# Move to Job and work
for _ in range(10):
    env.step(0)  # Move to Job (6,6)
env.step(4)  # Work

print(f'After working: {agent.meters.get(\"stress\").value} (expected: ~25)')

# Wait for passive decay
for _ in range(10):
    env.step(0)  # Just move around

print(f'After 10 steps: {agent.meters.get(\"stress\").value} (expected: ~24, -0.1 per step)')
"
```

---

## Training Comparison

### Recommended Training Run

```bash
# Backup old model
mv models/trained_agent.pt models/trained_agent_old_nostress.pt

# Train new agent with stress + exploit fix
uv run python demo_training.py

# Compare in web UI
# Old agent: Might interact-spam, no stress management
# New agent: Should navigate properly, balance work-recreation
```

### Metrics to Track

| Metric | Old Agent | New Agent (Expected) |
|--------|-----------|---------------------|
| Interact Spam | Yes | No (penalty prevents) |
| Job Visits/Episode | 2-3 | 4-5 (need money for recreation) |
| Recreation Visits | N/A | 2-3 (stress management) |
| Avg Stress Level | N/A | 20-40 (manageable) |
| Death by Stress | N/A | <5% (should learn to manage) |
| Overall Survival | 372 steps | 400+ steps (better strategy) |

---

## Files Modified

### Backend (Python)
- `src/hamlet/environment/meters.py`: Added Stress class
- `src/hamlet/environment/entities.py`: Added Recreation class, updated Job docstring
- `src/hamlet/environment/affordances.py`: Added Recreation effects, updated Job with stress
- `src/hamlet/environment/hamlet_env.py`:
  - Added Recreation to grid encoding (value 5.0, agent now 6.0)
  - Added stress gradient rewards (inverted logic)
  - Updated proximity shaping for stress (HIGH = bad)
  - Added failed interaction penalty (-1.0 energy)
- `src/hamlet/training/config.py`: Added Recreation position (3, 3)

### Frontend (Vue)
- `frontend/src/components/MeterPanel.vue`:
  - Added stress display formatting
  - Inverted color logic for stress
  - Critical threshold for high stress
- `frontend/src/components/Grid.vue`:
  - Added Recreation icon (ðŸŽ®)
  - Added Recreation styling (green)

### Documentation
- `docs/money_reward_improvements.md`: Existing
- `docs/stress_recreation_and_exploit_fix.md`: This file

---

## Next Steps

1. **Train agent** with new mechanics
2. **Observe behavior** in web UI:
   - Does it manage stress?
   - Does it use Recreation?
   - Does it navigate properly (no interact-spam)?
3. **Compare metrics** old vs. new
4. **Document findings** in teaching scraps
5. **Update configs** to save multiple checkpoints

---

## Pedagogical Applications

### Assignment 1: "Design the Recreation"
Give students current system without Recreation.
**Task**: Design an affordance and meter to prevent "work burnout"
**Compare**: To actual Recreation implementation

### Assignment 2: "Fix the Exploit"
Show old interact-spam behavior.
**Task**: Propose fixes without breaking learning
**Compare**: To actual penalty implementation

### Assignment 3: "Multi-Objective Optimization"
Give students the 5-meter system.
**Task**: Train agent, analyze trade-offs
**Report**: Work-life balance emergence

---

## Connection to Real AI Safety

**Work-life balance analogy**:
- Simple RL agent learns this intuitively
- Human organizations struggle with it constantly
- AI alignment: Building systems that optimize what we MEAN (balanced life) not what we MEASURE (productivity)

**The insight**: Even with perfect reward function, behavior emerges from optimization pressure. Recreation isn't programmed - it's discovered as necessary.

---

## Conclusion

These changes transform Hamlet from a **4-resource survival** simulation to a **5-resource work-life balance** simulation with proper navigation enforcement.

**Key improvements**:
1. âœ… Stress management adds strategic depth
2. âœ… Recreation creates interesting trade-offs
3. âœ… Exploit fix ensures proper navigation
4. âœ… Preserves pedagogical value of original exploit

**Result**: More realistic agent behaviors that mirror real-world resource management decisions.

**Next**: Train new agent and observe if it learns work-life balance!

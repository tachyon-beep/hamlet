# Level 2: Partial Observability (POMDP)
#
# See docs/TRAINING_LEVELS.md for complete specification.
#
# Key features:
# - Agent sees only 5×5 local window (partial observability)
# - RecurrentSpatialQNetwork with LSTM memory
# - Must build mental map through exploration
# - Target network for temporal credit assignment (ACTION #9)
# - NO proximity shaping (must learn through interaction)
# - Sparse rewards (milestone bonuses only)
#
# Expected performance:
# - 3000-5000 episodes to learn (vs 1000-2000 for Level 1)
# - Peak survival: 150-250 steps (vs 250-350 for Level 1)
# - 30-40% of time spent exploring
# - Realistic cognitive behavior (memory, mistakes, getting lost)

run_metadata:
  output_subdir: L2_partial_observability

# Environment configuration
environment:
  partial_observability: true  # LEVEL 2: Agent sees only 5×5 local window
  vision_range: 2  # 5×5 window (2*2+1)
  enable_temporal_mechanics: false  # LEVEL 2: No time-of-day (added in L3)
  enabled_affordances: null  # null = all 14 affordances enabled
  randomize_affordances: true
  energy_move_depletion: 0.005
  energy_wait_depletion: 0.001
  energy_interact_depletion: 0.0

# Network architecture is managed by brain.yaml:architecture.type
# ("feedforward" → SimpleQNetwork, "recurrent" → RecurrentSpatialQNetwork)
population:
  num_agents: 1  # Single agent for now
  mask_unused_obs: false  # Don't mask observations (standard behavior)

# Curriculum configuration (adversarial difficulty adjustment)
curriculum:
  max_steps_per_episode: 500
  survival_advance_threshold: 0.7  # Advance to next stage if 70% survival
  survival_retreat_threshold: 0.3  # Retreat if <30% survival
  entropy_gate: 0.5  # Minimum policy entropy to advance
  min_steps_at_stage: 1000  # Min episodes before stage change

# Exploration configuration (RND + adaptive annealing)
exploration:
  embed_dim: 128
  initial_intrinsic_weight: 0.1  # Much lower - let extrinsic milestones drive learning
  variance_threshold: 100.0  # Fixed: increased from 10.0 to prevent premature annealing
  min_survival_fraction: 0.4  # Don't anneal until mean survival >40% of max episode length (prevents "stable failure")
  survival_window: 100  # Track last 100 episodes for annealing
  epsilon_start: 1.0
  epsilon_min: 0.01
  epsilon_decay: 0.999  # Slow decay for thorough exploration

# Training configuration
training:
  device: cuda  # Use GPU if available
  max_episodes: 10000  # Total episodes for this training run

  # Q-learning hyperparameters
  train_frequency: 4  # Train Q-network every N steps
  batch_size: 16  # Smaller batch for LSTM (recurrent: 16, feedforward: 64)
  sequence_length: 8  # Length of sequences for LSTM training
  max_grad_norm: 10.0  # Gradient clipping threshold

  # Epsilon-greedy exploration (slower decay for POMDP)
  epsilon_start: 1.0  # Initial exploration rate
  epsilon_decay: 0.998  # Slow decay for POMDP (reaches ε=0.1 at ep 1150)
  epsilon_min: 0.01  # Minimum exploration rate

# Level 0.5: Dual Resource Management (Bed + Hospital)
#
# Intermediate step between L0 (single resource) and L1 (full environment).
#
# Key features:
# - 6×6 grid (slightly larger to fit 2 affordances with space)
# - TWO affordances: Bed (energy) + Hospital (health)
# - Agent must learn spatial navigation + resource prioritization
# - Full observability (agent sees entire grid)
# - Both affordances cost $3 (equal priority)
# - More starting money ($50 instead of $20) to sustain longer episodes
#
# Learning goals:
# - Interoception rewards create natural prioritization:
#   * Low energy + high health → ROI of Bed is HIGH
#   * High energy + low health → ROI of Hospital is HIGH
#   * Both low → Multiplicative penalty creates urgency
# - Agent should bounce between Bed and Hospital as needed
# - Eventually runs out of money and dies (sustainable cycles not possible without income)
#
# Expected convergence: ~200-300 episodes (2x longer than L0 due to exploration)

run_metadata:
  output_subdir: L0_5_dual_resource

# Environment configuration
environment:
  grid_size: 6  # 6×6 grid (36 cells) - bigger than L0 to fit 2 affordances
  partial_observability: false  # Agent sees full grid
  vision_range: 6  # Full vision (not used when partial_observability=false)
  enable_temporal_mechanics: false  # No time-of-day complexity

  # Only Bed and Hospital
  enabled_affordances: ["Bed", "Hospital","HomeMeal"]
  energy_move_depletion: 0.005
  energy_wait_depletion: 0.0049
  energy_interact_depletion: 0.0029

# Population configuration
population:
  num_agents: 1  # Single agent
  learning_rate: 0.001  # Higher learning rate for faster learning
  gamma: 0.95  # Slightly lower discount factor (shorter time horizon)
  replay_buffer_capacity: 5000  # Smaller buffer for minimal world
  network_type: simple  # Standard MLP Q-Network

# Curriculum configuration (simpler than Level 1)
curriculum:
  max_steps_per_episode: 1000  # Shorter episodes (5x5 world)
  survival_advance_threshold: 0.8  # Advance if 80% survival
  survival_retreat_threshold: 0.2  # Retreat if <20% survival
  entropy_gate: 0.3  # Lower entropy gate (simpler problem)
  min_steps_at_stage: 100  # Fewer steps before stage change

# Exploration configuration
exploration:
  embed_dim: 64  # Smaller embedding (simpler observations)
  initial_intrinsic_weight: 0.1  # Lower intrinsic weight (extrinsic is clearer)
  variance_threshold: 50.0  # Lower threshold for faster annealing
  survival_window: 50  # Smaller window

# Training configuration
training:
  device: cuda  # Use GPU if available
  max_episodes: 1000  # Should learn in <100, but allow buffer

  # Q-learning hyperparameters
  train_frequency: 4  # Train Q-network every N steps
  target_update_frequency: 100  # Update target network every N training steps
  batch_size: 64  # Batch size for experience replay (feedforward: 64, recurrent: 16)
  sequence_length: 8  # Length of sequences for LSTM training (recurrent only)
  max_grad_norm: 10.0  # Gradient clipping threshold

  # Epsilon-greedy exploration
  epsilon_start: 1.0  # Initial exploration rate
  epsilon_decay: 0.998  # Decay per episode
  epsilon_min: 0.01  # Minimum exploration rate

# Recording configuration
recording:
  enabled: true
  recordings_dir: "recordings"
  criteria:
    periodic:
      enabled: true
      interval: 50  # Record every 50th episode (shorter for testing)
    stage_transitions:
      enabled: true
      lookback: 3
      lookahead: 3
    performance:
      enabled: true
      top_percentile: 10.0
      bottom_percentile: 10.0
      history_window: 50
    stage_boundaries:
      enabled: true
      record_first_n: 3
      record_last_n: 3

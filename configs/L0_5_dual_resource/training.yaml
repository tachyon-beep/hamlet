# Level 0.5: Dual Resource Management (Bed + Hospital)
#
# Intermediate step between L0 (single resource) and L1 (full environment).
#
# Key features:
# - 6×6 grid (slightly larger to fit 2 affordances with space)
# - TWO affordances: Bed (energy) + Hospital (health)
# - Agent must learn spatial navigation + resource prioritization
# - Full observability (agent sees entire grid)
# - Both affordances cost $3 (equal priority)
# - More starting money ($50 instead of $20) to sustain longer episodes
#
# Learning goals:
# - Interoception rewards create natural prioritization:
#   * Low energy + high health → ROI of Bed is HIGH
#   * High energy + low health → ROI of Hospital is HIGH
#   * Both low → Multiplicative penalty creates urgency
# - Agent should bounce between Bed and Hospital as needed
# - Eventually runs out of money and dies (sustainable cycles not possible without income)
#
# Expected convergence: 200-400 episodes (max_episodes=500 provides buffer) (2x longer than L0 due to exploration)

run_metadata:
  output_subdir: L0_5_dual_resource

# Environment configuration
environment:
  partial_observability: false  # Agent sees full grid
  vision_range: 5  # Full vision (not used when partial_observability=false)
  enable_temporal_mechanics: false  # ENABLED: Multi-tick affordances (Bed=5 ticks, Job=4 ticks)

  # Bed, Hospital, HomeMeal, and Job (agent can earn money!)
  enabled_affordances: ["Bed", "Hospital","HomeMeal","Job"]
  randomize_affordances: true
  energy_move_depletion: 0.005
  energy_wait_depletion: 0.005
  energy_interact_depletion: 0.005

# Population configuration
population:
  num_agents: 1  # Single agent
  learning_rate: 0.0003  # Higher learning rate for faster learning
  gamma: 0.95  # Slightly lower discount factor (shorter time horizon)
  replay_buffer_capacity: 50000  # 50k capacity for better experience diversity (166-500 episodes)
  network_type: simple  # Standard MLP Q-Network

# Curriculum configuration (simpler than Level 1)
curriculum:
  max_steps_per_episode: 1000  # Shorter episodes for faster iteration (was 1000)
  survival_advance_threshold: 0.5  # Advance if 50% survival (was 0.8 - too high for L0.5)
  survival_retreat_threshold: 0.2  # Retreat if <20% survival
  entropy_gate: 0.5  # More forgiving entropy gate for early stages (was 0.3)
  min_steps_at_stage: 50  # Fewer steps before stage change (was 100)

# Exploration configuration
exploration:
  embed_dim: 128  # Smaller embedding (simpler observations)
  initial_intrinsic_weight: 0.1  # Higher for harder problem without REST
  variance_threshold: 100.0  # More forgiving variance (up from 50.0)
  min_survival_fraction: 0.15  # Don't anneal until mean survival >15% of max episode length (prevents "stable failure")
  survival_window: 200  # Smaller window

# Training configuration
training:
  device: cuda  # Use GPU if available
  max_episodes: 500  # Should converge in 200-300 episodes with fixed hyperparameters

  # Q-learning hyperparameters
  train_frequency: 4  # Train Q-network every N steps
  target_update_frequency: 200  # Update target network every N training steps
  batch_size: 64  # Batch size for experience replay (feedforward: 64, recurrent: 16)
  sequence_length: 8  # Length of sequences for LSTM training (recurrent only)
  max_grad_norm: 10.0  # Gradient clipping threshold
  use_double_dqn: false  # Vanilla DQN (for baseline comparison)

  # Epsilon-greedy exploration
  epsilon_start: 1.0  # Initial exploration rate
  epsilon_decay: 0.975  # Decay per episode (was 0.9995 - too slow, reaches ε=0.1 at ~460 episodes)
  epsilon_min: 0.01  # Minimum exploration rate
  enabled_actions:
    # L0.5 unlocks the full custom vocabulary (REST + MEDITATE) to pair with new affordances
    - "UP"
    - "DOWN"
    - "LEFT"
    - "RIGHT"
    - "INTERACT"
    - "WAIT"
    #- "REST"
    #- "MEDITATE"

# Recording configuration
recording:
  enabled: true
  recordings_dir: "recordings"
  criteria:
    periodic:
      enabled: true
      interval: 50  # Record every 50th episode (shorter for testing)
    stage_transitions:
      enabled: true
      lookback: 3
      lookahead: 3
    performance:
      enabled: true
      top_percentile: 10.0
      bottom_percentile: 10.0
      history_window: 50
    stage_boundaries:
      enabled: true
      record_first_n: 3
      record_last_n: 3

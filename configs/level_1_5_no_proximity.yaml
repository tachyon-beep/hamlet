# Level 1.5: Full Observability + NO Proximity Shaping
#
# This config removes the proximity reward hacking exploit by disabling
# proximity shaping. Agent must now actually interact with affordances
# to survive, not just stand near them.
#
# Expected behavior:
# - Agent learns to navigate and interact
# - No reward for proximity
# - Requires more exploration to find optimal strategies
# - Should take 1500-2000 episodes to converge

experiment:
  name: hamlet_level_1_5_no_proximity
  description: "Level 1.5: Full observability with NO proximity shaping - agent must interact"
  tracking_uri: "mlruns"

environment:
  grid_width: 8
  grid_height: 8

  # LEVEL 1.5: Full observability (baseline for comparison with Level 2)
  partial_observability: false  # Agent sees entire 8×8 grid
  vision_range: 2  # N/A for full observability

  # Reward mode: shaped but NO proximity
  # (proximity shaping is hardcoded to False in hamlet_env.py)
  reward_mode: shaped

  # Initial meter values
  initial_energy: 100.0
  initial_hygiene: 100.0
  initial_satiation: 100.0
  initial_money: 50.0
  initial_mood: 100.0
  initial_social: 50.0

  # Depletion rates
  energy_depletion: 0.5
  hygiene_depletion: 0.3
  satiation_depletion: 0.4
  money_depletion: 0.0
  mood_depletion: 0.1
  social_depletion: 0.6

  # Affordance positions (x, y)
  affordance_positions:
    Bed: [1, 1]
    Shower: [2, 2]
    HomeMeal: [1, 3]
    FastFood: [5, 6]
    Job: [6, 6]
    Gym: [7, 3]
    Bar: [7, 0]
    Recreation: [0, 7]

agents:
  - agent_id: agent_0
    algorithm: dqn
    state_dim: 72  # Full 8×8 grid observation
    action_dim: 5
    learning_rate: 0.00025
    gamma: 0.99
    epsilon: 1.0
    epsilon_min: 0.01
    epsilon_decay: 0.995
    device: auto
    network_type: qnetwork  # Standard MLP for full observability
    grid_size: 8

training:
  num_episodes: 2000  # More episodes without proximity hints
  max_steps_per_episode: 500
  batch_size: 64
  learning_starts: 1000
  target_update_frequency: 100
  replay_buffer_size: 10000

  # Checkpointing every 100 episodes
  save_frequency: 100
  checkpoint_dir: "checkpoints_level_1_5/"

  # Logging
  log_frequency: 10
  render_mode: null

metrics:
  tensorboard: true
  tensorboard_dir: "runs_level_1_5"
  database: true
  database_path: "metrics_level_1_5.db"
  replay_storage: false
  live_broadcast: false

# training.yaml - TEMPLATE
#
# Copy this file into your config pack directory and customise the values below.
# Every pack should include:
#   - training.yaml          (this file)
#   - affordances.yaml       (affordance definitions)
#   - bars.yaml              (meter definitions)
#   - cascades.yaml          (cascade physics)
#   - cues.yaml              (observable cues, optional for now)
#
# The `run_metadata.output_subdir` controls where run outputs are saved under
# the `runs/` directory. Use a short, filesystem-safe name (no spaces).
#
# Environment/population/curriculum/exploration values are the same ones used
# in the previous single-file configs. Feel free to remove keys you don't need;
# defaults in the training code will be used when available.

run_metadata:
  output_subdir: PACK_NAME_HERE

environment:
  partial_observability: false
  vision_range: 8
  enable_temporal_mechanics: false
  enabled_affordances: null
  randomize_affordances: true
  energy_move_depletion: 0.005
  energy_wait_depletion: 0.001
  energy_interact_depletion: 0.0

population:
  num_agents: 1
  learning_rate: 0.00025
  gamma: 0.99
  replay_buffer_capacity: 10000
  network_type: simple
  mask_unused_obs: false  # Don't mask observations (standard behavior)

curriculum:
  max_steps_per_episode: 500
  survival_advance_threshold: 0.7
  survival_retreat_threshold: 0.3
  entropy_gate: 0.5
  min_steps_at_stage: 1000

exploration:
  embed_dim: 128
  initial_intrinsic_weight: 1.0
  variance_threshold: 100.0
  min_survival_fraction: 0.4  # Don't anneal until mean survival >40% of max episode length (prevents "stable failure")
  survival_window: 100

training:
  device: cuda
  max_episodes: 5000

  # Q-learning hyperparameters
  train_frequency: 4
  target_update_frequency: 100
  batch_size: 64
  sequence_length: 8
  max_grad_norm: 10.0
  use_double_dqn: false  # Vanilla DQN (for baseline comparison)

  # Reward strategy
  reward_strategy: multiplicative  # 'multiplicative' (original) or 'adaptive' (fixes Low Energy Delirium bug)

  # Epsilon-greedy exploration
  epsilon_start: 1.0
  epsilon_decay: 0.995
  epsilon_min: 0.01

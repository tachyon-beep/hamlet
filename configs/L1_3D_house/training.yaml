# Level 1: Full Observability Baseline
#
# See docs/TRAINING_LEVELS.md for complete specification.
#
# Key features:
# - Agent sees full 8×8 grid (complete information)
# - Standard MLP Q-Network (no memory needed)
# - NO proximity shaping (must learn through interaction)
# - Sparse rewards (milestone bonuses only)
# - Adversarial curriculum (5-stage progressive difficulty)
#
# Expected performance:
# - 1000-2000 episodes to learn
# - Peak survival: 250-350 steps
# - Faster learning than Level 2 (has full information)
# - Clean baseline for comparing POMDP performance

run_metadata:
  output_subdir: L1_full_observability

# Environment configuration
environment:
  partial_observability: false  # LEVEL 1: Agent sees full grid
  vision_range: 8  # Full vision (not used when partial_observability=false)
  enable_temporal_mechanics: false  # No time-based mechanics
  enabled_affordances: null  # null = all 14 affordances enabled
  randomize_affordances: true
  energy_move_depletion: 0.005  # Energy cost per movement action
  energy_wait_depletion: 0.001  # Energy cost per WAIT action
  energy_interact_depletion: 0.0  # Energy cost per INTERACT action

# Population configuration
population:
  num_agents: 1  # Single agent for now
  learning_rate: 0.00025  # Standard Atari DQN learning rate
  gamma: 0.99
  replay_buffer_capacity: 10000
  network_type: simple  # Standard MLP Q-Network
  mask_unused_obs: false  # Don't mask observations (standard behavior)

# Curriculum configuration (adversarial difficulty adjustment)
curriculum:
  max_steps_per_episode: 500
  survival_advance_threshold: 0.7  # Advance to next stage if 70% survival
  survival_retreat_threshold: 0.3  # Retreat if <30% survival
  entropy_gate: 0.5  # Minimum policy entropy to advance
  min_steps_at_stage: 1000  # Min episodes before stage change

# Exploration configuration (RND + adaptive annealing)
exploration:
  embed_dim: 128
  initial_intrinsic_weight: 1.0
  variance_threshold: 100.0  # Fixed: increased from 10.0 to prevent premature annealing
  min_survival_fraction: 0.4  # Don't anneal until mean survival >40% of max episode length (prevents "stable failure")
  survival_window: 100  # Track last 100 episodes for annealing

# Training configuration
training:
  device: cuda  # Use GPU if available
  max_episodes: 5000  # Total episodes for this training run (Level 1.5 learns faster)

  # Q-learning hyperparameters
  train_frequency: 4  # Train Q-network every N steps
  target_update_frequency: 100  # Update target network every N training steps
  batch_size: 64  # Batch size for experience replay (feedforward: 64, recurrent: 16)
  sequence_length: 8  # Length of sequences for LSTM training (recurrent only)
  max_grad_norm: 10.0  # Gradient clipping threshold
  use_double_dqn: false  # Vanilla DQN (for baseline comparison)

  # Reward strategy
  reward_strategy: multiplicative  # 'multiplicative' (original) or 'adaptive' (fixes Low Energy Delirium bug)

  # Epsilon-greedy exploration
  epsilon_start: 1.0  # Initial exploration rate
  epsilon_decay: 0.995  # Decay per episode (reaches ε=0.1 at ~460 episodes)
  epsilon_min: 0.01  # Minimum exploration rate

# Hamlet Training Configuration - Dueling DQN with Shaped Rewards
# This configuration uses dueling architecture without spatial CNN

experiment:
  name: hamlet_dueling_shaped
  description: "Dueling DQN with hybrid reward shaping, standard MLP architecture"
  tracking_uri: "mlruns"

environment:
  grid_width: 8
  grid_height: 8
  initial_energy: 100.0
  initial_hygiene: 100.0
  initial_satiation: 100.0
  initial_money: 50.0
  energy_depletion: 0.5
  hygiene_depletion: 0.3
  satiation_depletion: 0.4
  affordance_positions:
    Bed: [1, 1]
    Shower: [6, 1]
    Fridge: [1, 6]
    Job: [6, 6]

agents:
  - agent_id: agent_0
    algorithm: dqn
    state_dim: 70
    action_dim: 5
    learning_rate: 0.00025  # Atari DQN standard
    gamma: 0.99
    epsilon: 1.0
    epsilon_min: 0.01
    epsilon_decay: 0.995
    device: auto
    network_type: dueling  # Value/advantage separation without spatial processing
    grid_size: 8

training:
  num_episodes: 1000
  max_steps_per_episode: 500
  batch_size: 64
  learning_starts: 1000
  target_update_frequency: 100
  replay_buffer_size: 10000
  save_frequency: 100
  checkpoint_dir: "checkpoints_dueling/"
  log_frequency: 10

metrics:
  tensorboard: true
  tensorboard_dir: "runs_dueling"
  database: true
  database_path: "metrics_dueling.db"
  replay_storage: false
  replay_dir: "replays"
  replay_sample_rate: 0.1
  live_broadcast: false

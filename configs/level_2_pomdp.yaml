# Level 2: Partial Observability (POMDP) + Recurrent Network
#
# This config implements Level 2 from ARCHITECTURE_DESIGN.md:
# - Agent sees only 5×5 local window (partial observability)
# - Must build mental map through exploration
# - Uses LSTM to remember where affordances are located
# - NO proximity shaping (must learn through interaction)
#
# Expected behavior:
# - 2000-3000 episodes to learn (vs 1500-2000 for full obs)
# - Peak reward: +30 to +50 (vs +52 full obs)
# - 30-40% of time spent exploring
# - Realistic cognitive behavior (memory, mistakes)

experiment:
  name: hamlet_level_2_pomdp
  description: "Level 2: Partial observability (5×5 window) with LSTM memory"
  tracking_uri: "mlruns"

environment:
  grid_width: 8
  grid_height: 8

  # LEVEL 2: Partial observability (POMDP)
  partial_observability: true  # Agent sees only 5×5 local window
  vision_range: 2  # 5×5 window (2*2+1)

  # Reward mode: shaped but NO proximity
  reward_mode: shaped

  # Initial meter values
  initial_energy: 100.0
  initial_hygiene: 100.0
  initial_satiation: 100.0
  initial_money: 50.0
  initial_mood: 100.0
  initial_social: 50.0

  # Depletion rates
  energy_depletion: 0.5
  hygiene_depletion: 0.3
  satiation_depletion: 0.4
  money_depletion: 0.0
  mood_depletion: 0.1
  social_depletion: 0.6

  # Affordance positions (same as Level 1.5 for fair comparison)
  affordance_positions:
    Bed: [1, 1]
    Shower: [2, 2]
    HomeMeal: [1, 3]
    FastFood: [5, 6]
    Job: [6, 6]
    Gym: [7, 3]
    Bar: [7, 0]
    Recreation: [0, 7]

agents:
  - agent_id: agent_0
    algorithm: dqn
    state_dim: 33  # 5×5 grid (25) + 2 position + 8 meters = 35 (LSTM handles rest)
    action_dim: 5
    learning_rate: 0.0001  # Lower LR for recurrent networks
    gamma: 0.99
    epsilon: 1.0
    epsilon_min: 0.05  # Slightly higher min for exploration
    epsilon_decay: 0.997  # Slower decay for exploration phase
    device: auto
    network_type: recurrent  # NEW: RecurrentSpatialQNetwork
    grid_size: 5  # 5×5 local window

training:
  num_episodes: 3000  # More episodes for POMDP (needs exploration)
  max_steps_per_episode: 500
  batch_size: 32  # Smaller batch for recurrent training
  learning_starts: 2000  # More warmup for memory building
  target_update_frequency: 100
  replay_buffer_size: 10000

  # Checkpointing every 100 episodes
  save_frequency: 100
  checkpoint_dir: "checkpoints_level_2/"

  # Logging
  log_frequency: 10
  render_mode: null

metrics:
  tensorboard: true
  tensorboard_dir: "runs_level_2"
  database: true
  database_path: "metrics_level_2.db"
  replay_storage: false
  live_broadcast: false
